{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar = pd.read_csv('../data/calendar.csv')\n",
    "sales = pd.read_csv('../data/sales_train_evaluation.csv')\n",
    "price = pd.read_csv('../data/sell_prices.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill in the columns to be predicted\n",
    "History data : d_1 ~ d_1913\n",
    "\n",
    "Predict:\n",
    "\n",
    "- Validation : d_1914 ~ d_1941\n",
    "- Evaluation : d_1942 ~ d_1969"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>d_1</th>\n",
       "      <th>d_2</th>\n",
       "      <th>d_3</th>\n",
       "      <th>d_4</th>\n",
       "      <th>...</th>\n",
       "      <th>d_1960</th>\n",
       "      <th>d_1961</th>\n",
       "      <th>d_1962</th>\n",
       "      <th>d_1963</th>\n",
       "      <th>d_1964</th>\n",
       "      <th>d_1965</th>\n",
       "      <th>d_1966</th>\n",
       "      <th>d_1967</th>\n",
       "      <th>d_1968</th>\n",
       "      <th>d_1969</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_002</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_003_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_003</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_004_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_004</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_005_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_005</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30485</th>\n",
       "      <td>FOODS_3_823_WI_3_evaluation</td>\n",
       "      <td>FOODS_3_823</td>\n",
       "      <td>FOODS_3</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>WI_3</td>\n",
       "      <td>WI</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30486</th>\n",
       "      <td>FOODS_3_824_WI_3_evaluation</td>\n",
       "      <td>FOODS_3_824</td>\n",
       "      <td>FOODS_3</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>WI_3</td>\n",
       "      <td>WI</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30487</th>\n",
       "      <td>FOODS_3_825_WI_3_evaluation</td>\n",
       "      <td>FOODS_3_825</td>\n",
       "      <td>FOODS_3</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>WI_3</td>\n",
       "      <td>WI</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30488</th>\n",
       "      <td>FOODS_3_826_WI_3_evaluation</td>\n",
       "      <td>FOODS_3_826</td>\n",
       "      <td>FOODS_3</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>WI_3</td>\n",
       "      <td>WI</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30489</th>\n",
       "      <td>FOODS_3_827_WI_3_evaluation</td>\n",
       "      <td>FOODS_3_827</td>\n",
       "      <td>FOODS_3</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>WI_3</td>\n",
       "      <td>WI</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30490 rows Ã— 1975 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  id        item_id    dept_id   cat_id  \\\n",
       "0      HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES   \n",
       "1      HOBBIES_1_002_CA_1_evaluation  HOBBIES_1_002  HOBBIES_1  HOBBIES   \n",
       "2      HOBBIES_1_003_CA_1_evaluation  HOBBIES_1_003  HOBBIES_1  HOBBIES   \n",
       "3      HOBBIES_1_004_CA_1_evaluation  HOBBIES_1_004  HOBBIES_1  HOBBIES   \n",
       "4      HOBBIES_1_005_CA_1_evaluation  HOBBIES_1_005  HOBBIES_1  HOBBIES   \n",
       "...                              ...            ...        ...      ...   \n",
       "30485    FOODS_3_823_WI_3_evaluation    FOODS_3_823    FOODS_3    FOODS   \n",
       "30486    FOODS_3_824_WI_3_evaluation    FOODS_3_824    FOODS_3    FOODS   \n",
       "30487    FOODS_3_825_WI_3_evaluation    FOODS_3_825    FOODS_3    FOODS   \n",
       "30488    FOODS_3_826_WI_3_evaluation    FOODS_3_826    FOODS_3    FOODS   \n",
       "30489    FOODS_3_827_WI_3_evaluation    FOODS_3_827    FOODS_3    FOODS   \n",
       "\n",
       "      store_id state_id  d_1  d_2  d_3  d_4  ...  d_1960  d_1961  d_1962  \\\n",
       "0         CA_1       CA    0    0    0    0  ...     NaN     NaN     NaN   \n",
       "1         CA_1       CA    0    0    0    0  ...     NaN     NaN     NaN   \n",
       "2         CA_1       CA    0    0    0    0  ...     NaN     NaN     NaN   \n",
       "3         CA_1       CA    0    0    0    0  ...     NaN     NaN     NaN   \n",
       "4         CA_1       CA    0    0    0    0  ...     NaN     NaN     NaN   \n",
       "...        ...      ...  ...  ...  ...  ...  ...     ...     ...     ...   \n",
       "30485     WI_3       WI    0    0    2    2  ...     NaN     NaN     NaN   \n",
       "30486     WI_3       WI    0    0    0    0  ...     NaN     NaN     NaN   \n",
       "30487     WI_3       WI    0    6    0    2  ...     NaN     NaN     NaN   \n",
       "30488     WI_3       WI    0    0    0    0  ...     NaN     NaN     NaN   \n",
       "30489     WI_3       WI    0    0    0    0  ...     NaN     NaN     NaN   \n",
       "\n",
       "       d_1963  d_1964  d_1965  d_1966  d_1967  d_1968  d_1969  \n",
       "0         NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
       "1         NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
       "2         NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
       "3         NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
       "4         NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
       "...       ...     ...     ...     ...     ...     ...     ...  \n",
       "30485     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
       "30486     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
       "30487     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
       "30488     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
       "30489     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
       "\n",
       "[30490 rows x 1975 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_fill_date = sales.copy()\n",
    "date_list = []\n",
    "for i in range(1942, 1970):\n",
    "    date_list.append('d_'+str(i))\n",
    "fill_date = pd.DataFrame(columns = date_list)\n",
    "sales_fill_date = pd.concat([sales_fill_date, fill_date], axis = 1)\n",
    "sales_fill_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slice dataset into smaller groups for EDA & prediction demo\n",
    "- cat_id : FOODS\n",
    "- store_id : CA_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_group_df(group_sales, df_calendar, df_price):\n",
    "    # Transpose 'd_1' to 'd_1969' columns of group_sales\n",
    "    group_sales = pd.melt(group_sales, id_vars=group_sales.columns[0:6], \n",
    "                          value_vars=group_sales.columns[6:], var_name='d', value_name='sales')\n",
    "    \n",
    "    # Merge group_sales with calendar & price\n",
    "    df_group = pd.merge(group_sales, calendar, how='left', on='d')\n",
    "    df_group = pd.merge(df_group, price, how='left')\n",
    "    \n",
    "    # Extract necessary columns\n",
    "    df_group = df_group[['id','item_id','dept_id','cat_id','d','sales','wm_yr_wk','event_name_1',\n",
    "                 'event_type_1','event_name_2','event_type_2','snap_CA','snap_TX','snap_WI']].copy()\n",
    "    df_group['d_new'] = df_group['d'].str[2:].astype('int')\n",
    "    df_group['week'] = df_group['wm_yr_wk']-11100\n",
    "    df_group = df_group.sort_values(['id', 'd_new']).reset_index(drop=True)\n",
    "    return df_group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset for prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Reference of LSTM Model:\n",
    "#### https://machinelearningmastery.com/time-series-forecasting-long-short-term-memory-network-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a differenced series\n",
    "def difference(dataset, interval=1):\n",
    "\tdiff = list()\n",
    "\tfor i in range(interval, len(dataset)):\n",
    "\t\tvalue = dataset[i] - dataset[i - interval]\n",
    "\t\tdiff.append(value)\n",
    "\treturn pd.Series(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frame a sequence as a supervised learning problem(fill NaN with 0)\n",
    "def timeseries_to_supervised_1(data, lag=365):\n",
    "\tdf = pd.DataFrame(data)\n",
    "\tcolumns = [df.shift(i) for i in range(1, lag+1)]\n",
    "\tcolumns.append(df)\n",
    "\tdf = pd.concat(columns, axis=1)\n",
    "\tdf.fillna(0, inplace=True)\n",
    "\treturn df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frame a sequence as a supervised learning problem(drop rows with NaN)\n",
    "def timeseries_to_supervised_2(data, lag=365):\n",
    "\tdf = pd.DataFrame(data)\n",
    "\tcolumns = [df.shift(i) for i in range(1, lag+1)]\n",
    "\tcolumns.append(df)\n",
    "\tdf = pd.concat(columns, axis=1)\n",
    "\tdf = df.dropna()\n",
    "\treturn df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale train and test data to [-1, 1]\n",
    "def scale(train, test=np.array([])):\n",
    "\t# fit scaler\n",
    "\tscaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\tscaler = scaler.fit(train)\n",
    "\t# transform train\n",
    "\ttrain = train.reshape(train.shape[0], train.shape[1])\n",
    "\ttrain_scaled = scaler.transform(train)\n",
    "\tif test.size != 0:\n",
    "\t    # transform test\n",
    "\t    test = test.reshape(test.shape[0], test.shape[1])\n",
    "\t    test_scaled = scaler.transform(test)\n",
    "\telse: \n",
    "\t    test_scaled = test\n",
    "\treturn scaler, train_scaled, test_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit an LSTM network to training data\n",
    "def fit_lstm(train, batch_size, nb_epoch, neurons):\n",
    "\tX, y = train[:, 0:-1], train[:, -1]\n",
    "\tX = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(LSTM(neurons, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\n",
    "\tmodel.add(Dense(1))\n",
    "\tmodel.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    # Revise the way of model training to reset state for each epoch\n",
    "\tfor i in range(nb_epoch):\n",
    "\t\tmodel.fit(X, y, epochs=1, batch_size=batch_size, verbose=0, shuffle=False)\n",
    "\t\tmodel.reset_states()\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction based on the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a one-step forecast\n",
    "def forecast_lstm(model, batch_size, X):\n",
    "\tX = X.reshape(1, 1, len(X))\n",
    "\tyhat = model.predict(X, batch_size=batch_size)\n",
    "\treturn yhat[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverse scaling for a forecasted value\n",
    "def invert_scale(scaler, X, value):\n",
    "\tnew_row = [x for x in X] + [value]\n",
    "\tarray = np.array(new_row)\n",
    "\tarray = array.reshape(1, len(array))\n",
    "\tinverted = scaler.inverse_transform(array)\n",
    "\treturn inverted[0, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invert differenced value\n",
    "def inverse_difference(history, yhat, interval=1):\n",
    "\treturn yhat + history[-interval]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict all item sales\n",
    "\n",
    "Total: 30 models (3 categories * 10 stores)\n",
    "\n",
    "Predict each product (each id) by using it's groups model.\n",
    "\n",
    "Time: about 4 hours\n",
    "\n",
    "Score: 1.1214"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CA_1 HOBBIES\n",
      "CA_2 HOBBIES\n",
      "CA_3 HOBBIES\n",
      "CA_4 HOBBIES\n",
      "TX_1 HOBBIES\n",
      "TX_2 HOBBIES\n",
      "TX_3 HOBBIES\n",
      "WI_1 HOBBIES\n",
      "WI_2 HOBBIES\n",
      "WI_3 HOBBIES\n",
      "CA_1 HOUSEHOLD\n",
      "CA_2 HOUSEHOLD\n",
      "CA_3 HOUSEHOLD\n",
      "CA_4 HOUSEHOLD\n",
      "TX_1 HOUSEHOLD\n"
     ]
    }
   ],
   "source": [
    "final_val = pd.DataFrame()\n",
    "final_eval = pd.DataFrame()\n",
    "\n",
    "# slice dataset into smaller groups\n",
    "for cat in sales_fill_date['cat_id'].unique():\n",
    "    for store in sales_fill_date['store_id'].unique():\n",
    "        print(store, cat)\n",
    "        df_sales = sales_fill_date[(sales_fill_date['cat_id']==cat) & (sales_fill_date['store_id']==store)]\n",
    "        df_group = create_group_df(df_sales, calendar, price)\n",
    "\n",
    "        # form the sales dataset\n",
    "        df_group_ttl_sales = df_group[['d_new','sales']].groupby(['d_new'])['sales'].sum().reset_index()[:-28]\n",
    "        group_ttl_sales = df_group_ttl_sales.sales.values\n",
    "\n",
    "        # transform data to be stationary\n",
    "        group_diff = difference(group_ttl_sales, 1)\n",
    "\n",
    "        # transform data to be supervised learning by timeseries_to_supervised_1 function\n",
    "        group_supervised = timeseries_to_supervised_2(group_diff, 365).values\n",
    "\n",
    "        # split data into training set\n",
    "        train = group_supervised[0:-56]\n",
    "\n",
    "        # transform the scale of the data\n",
    "        scaler, train_scaled, test_scaled = scale(train)\n",
    "\n",
    "        # fit the model\n",
    "        lstm_model = fit_lstm(train_scaled, 1, 100, 4)\n",
    "\n",
    "        # forecast the entire training dataset to build up state for forecasting\n",
    "        train_reshaped = train_scaled[:,:-1].reshape(len(train_scaled), 1, 365)\n",
    "        lstm_model.predict(train_reshaped, batch_size=1)\n",
    "\n",
    "        # slice dataset into item data\n",
    "        for ID in df_group['id'].unique():\n",
    "            item_sales = df_group[df_group['id']==ID].copy().reset_index(drop = True).sales.values[:-28]\n",
    "            \n",
    "            # transform data to be stationary\n",
    "            item_diff = difference(item_sales, 1)\n",
    "\n",
    "            # transform data to be supervised learning by timeseries_to_supervised_1 function\n",
    "            item_supervised = timeseries_to_supervised_2(item_diff, 365).values\n",
    "\n",
    "            # slice data for forecasting\n",
    "            forecast = item_supervised[-56:]\n",
    "\n",
    "            # transform the scale of the data\n",
    "            scaler, forecast_scaled, test_scaled = scale(forecast)\n",
    "            \n",
    "            # walk-forward validation on the forecast data\n",
    "            predictions = list()\n",
    "            for i in range(len(forecast_scaled)): # make one forecast for each day\n",
    "                # make one-step forecast\n",
    "                X = forecast_scaled[i,:-1]\n",
    "                yhat = forecast_lstm(lstm_model, 1, X)\n",
    "                # invert scaling\n",
    "                yhat = invert_scale(scaler, X, yhat)\n",
    "                # invert differencing\n",
    "                yhat = inverse_difference(item_sales, yhat, len(forecast_scaled)+1-i)\n",
    "                # store forecast\n",
    "                predictions.append(yhat)\n",
    "            \n",
    "            result_list = []\n",
    "            for i in range(0,len(predictions)):\n",
    "                result_dict = {}\n",
    "                result_dict['pred_value'] = int(predictions[i])\n",
    "                result_list.append(result_dict)\n",
    "            df_result = pd.DataFrame(result_list)\n",
    "        \n",
    "            final_val = final_val.append(np.transpose(pd.DataFrame([ID] + list(df_result['pred_value'].iloc[0:28,]))))\n",
    "            final_eval = final_eval.append(np.transpose(pd.DataFrame([ID] + list(df_result['pred_value'].iloc[28:,])))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final submission processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_colnames = ['id']\n",
    "for i in range(1,29):\n",
    "    submission_colnames.append('F'+str(i))\n",
    "submission_all = pd.DataFrame(columns = submission_colnames)\n",
    "\n",
    "final_val.columns = submission_colnames\n",
    "final_eval.columns = submission_colnames\n",
    "final_val['id'] = final_val['id'].str.replace('evaluation','validation')\n",
    "\n",
    "submission = pd.concat([final_val, final_eval], axis = 0).reset_index(drop = True)\n",
    "submission = submission.clip(lower=0)\n",
    "submission_all = pd.concat([submission_all, submission], axis = 0)\n",
    "submission_all.to_csv('submission_lstm_ver2.csv', index = False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
